{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a7180a-6b4c-4a97-9d13-35647c2f094c",
   "metadata": {},
   "source": [
    "# Lab 7c: Simple Games - Traveler’s Dilemma\n",
    "The Traveler’s Dilemma is a non-cooperative __non-zero sum game__ that involves an airline losing two identical suitcases belonging to two travelers. The airline requests that the travelers report the value of their suitcases, which must fall in the range of `2 USD` and `100 USD`, in increments of $\\pm$ `1 USD`.\n",
    "\n",
    "__Rules__:\n",
    "* If both travelers report the same value, they receive that value as a reward. \n",
    "* However, if the travelers report different values, the traveler with the lower value receives their reported value plus an additional `2 USD`. In comparison, the traveler with the higher value receives the lower value minus `2 USD`. \n",
    "\n",
    "The reward function is determined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "R_{i}(a_{i},a_{-i}) = \n",
    "\\begin{cases}\n",
    "a_{i} & \\text{if } a_{i} = a_{-i} \\\\\n",
    "a_{i} + 2 & \\text{if } a_{i} < a_{-i} \\\\\n",
    "a_{-1} - 2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Most people put down between `97 USD` and `100 USD`. However, somewhat counter-intuitively, there is a unique [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) of only `2 USD`.\n",
    "\n",
    "### Learning objectives\n",
    "The objective of `Lab 7c` is to familiarize students with the [Traveler’s Dilemma problem](https://en.wikipedia.org/wiki/Traveler%27s_dilemma), the solution of the problem using iterative refinement and the concept of [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium).\n",
    "\n",
    "* The [Traveler’s Dilemma problem](https://en.wikipedia.org/wiki/Traveler%27s_dilemma) was first posed by [Kaushik Basu](https://en.wikipedia.org/wiki/Kaushik_Basu), a Cornell professor and former Chief Economist of the World Bank (2012 - 2016). For more information on this problem (beyond what is described here), check out this [article](https://www.academia.edu/56129718/The_Travelers_Dilemma). This problem (as we shall see) has a [Nash Equilibrium solution](https://en.wikipedia.org/wiki/Nash_equilibrium) of `2 USD`.\n",
    "* In [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium), each player is assumed to know the equilibrium strategies of the other players, and no single player can gain by changing only their strategy.\n",
    "\n",
    "#### Sources\n",
    "The implementation of the Traveler’s Dilemma problem was taken from the `Decisions` book:\n",
    "\n",
    "* [Algorithms For Decision Making, Kochenderfer, Wheeler, Wray, MIT Press, 2022](https://algorithmsbook.com)\n",
    "\n",
    "We've implemented some of the codes found in `Chapter 24` of the `Decisions` book in our package [VLDecisionsPackage.jl](https://github.com/varnerlab/VLDecisionsPackage.jl.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfaffc-a639-4965-b29a-b20bad0e0cb8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The computations in this lab (or example) are enabled by the [VLDecisionsPackage.jl](https://github.com/varnerlab/VLDecisionsPackage.jl.git) and several external `Julia` packages. To load the required packages and any custom codes the teaching team has developed to work with these packages, we [include](https://docs.julialang.org/en/v1/manual/code-loading/) the `Include.jl` file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ab7b7-08b4-46b4-80ec-8c848beb412b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Manifest.toml`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPDMats\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mDistributions\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions → DistributionsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelDensity\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mVLDecisionsPackage\n",
      "\u001b[32m  ✓ \u001b[39mStatsPlots\n",
      "\u001b[32m  ✓ \u001b[39mVLQuantitativeFinancePackage\n",
      "  7 dependencies successfully precompiled in 10 seconds. 252 already precompiled.\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Desktop/julia_work/PS4-CHEME-5760-TravelerDilemma-Fall-2023/Manifest.toml`\n",
      "  \u001b[90m[90014a1f] \u001b[39m\u001b[93m↑ PDMats v0.11.25 ⇒ v0.11.26\u001b[39m\n",
      "  \u001b[90m[5c2747f8] \u001b[39m\u001b[93m↑ URIs v1.5.0 ⇒ v1.5.1\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115e5b2-0b51-411f-a794-9d8ac862725c",
   "metadata": {},
   "source": [
    "## Task 1: Build a `MySimpleGameModel` instance\n",
    "\n",
    "#### Model\n",
    "Let's begin `Lab 7c` by constructing a model of the game, which is an instance of the `MySimpleGameModel` type:\n",
    "\n",
    "```julia\n",
    "# The game model holds values (and functions) that are useful to evaluate the game\n",
    "mutable struct MySimpleGameModel <: AbstractGameModel\n",
    "\n",
    "    # data -\n",
    "    γ   # discount factor -\n",
    "    ℐ   # set of players -\n",
    "    𝒜   # joint action space\n",
    "    R   # joint reward function\n",
    "\n",
    "    # # constructor -\n",
    "    MySimpleGameModel() = new();\n",
    "end\n",
    "```\n",
    "\n",
    "Instances of the `MySimpleGameModel` type have the following fields:\n",
    "\n",
    "* The `γ::Float64` field holds the discount factor for the game (the weight of current versus future rewards, not used in this game).\n",
    "* The `ℐ::Array{Int64,1}` field holds the list of players, in our case `{1,2}`\n",
    "* The `𝒜` field holds the joint action space for the game (the collection of actions for each player)\n",
    "* The `R::Function` field holds the joint reward function $R(a)$: this method is called with a joint action $a$ and returns a reward.\n",
    "\n",
    "#### Build\n",
    "We build a game model by passing the type of game we want to construct, in this case a `MyTravelersProblem`, into a the `build(...)` method:\n",
    "\n",
    "```julia\n",
    "function build(simpleGame::Type{MyTravelersProblem})\n",
    "\n",
    "    # build an empty model -\n",
    "    model = MySimpleGameModel();\n",
    "    \n",
    "    # populate the model -\n",
    "    model.γ = 0.9;\n",
    "    model.ℐ = vec(collect(1:n_agents(simpleGame)))\n",
    "    model.𝒜 = [ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)]\n",
    "    model.R = (a) -> joint_reward(simpleGame, a)\n",
    "\n",
    "    # return the model -\n",
    "    return model;\n",
    "end\n",
    "```\n",
    "\n",
    "The `build(...)` method constructs an empty model, then populates the model with the required data. Save your instance of the game model in the `mysimplemodel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f447f6-864e-455d-a702-e892534edd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mysimplemodel = build(MyTravelersProblem);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30833666-e0e7-4223-9bde-3d6641b6f897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Compute the iterated best policy for the Traveler’s Dilemma problem\n",
    "\n",
    "The iterated best policy is computed using the `solve(...)` method, given by:\n",
    "\n",
    "```julia\n",
    "function solve(M::MyIteratedBestResponsePolicy, 𝒫::MySimpleGameModel)\n",
    "    π = M.π\n",
    "    for _ in 1:M.k_max\n",
    "        π = [best_response_policy(𝒫, π, i) for i in 𝒫.ℐ]\n",
    "    end\n",
    "    return π\n",
    "end\n",
    "```\n",
    "\n",
    "The `solve(...)` method takes the `MyIteratedBestResponsePolicy` and `MySimpleGameModel` instances and returns the best response policy for the Traveler’s Dilemma problem, computed by iterative refinement. The updates continue for `k_max` iterations, where during each iteration:\n",
    "\n",
    "* The joint policy $\\pi$ is updated for each player $i\\in\\left\\{1,2\\right\\}$ using an [array comprehension](https://docs.julialang.org/en/v1/manual/arrays/#man-comprehensions) operation. The update calls the `best_response_policy(...)` function, which returns the deterministic best policy.\n",
    "* After `k_max` updates, the refined joint policy $\\pi$ is returned to the caller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d047b3-252c-47cf-afa9-cdc5dd637a5e",
   "metadata": {},
   "source": [
    "### Initialize a uniformly random policy\n",
    "To construct an initial policy, we use the `build(...)` method to build and initialize an `MyIteratedBestResponsePolicy` instance. The `build(...)` method is given by:\n",
    "\n",
    "```julia\n",
    "function build(𝒫::MySimpleGameModel, k_max)\n",
    "    π = [MySimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in 𝒫.𝒜]\n",
    "    return MyIteratedBestResponsePolicy(k_max, π)\n",
    "end\n",
    "```\n",
    "\n",
    "The `build(...)` method takes a `MySimpleGameModel` instance, and a value for the `k_max` parameter and returns a `MyIteratedBestResponsePolicy` with a uniform policy, which is stored in the `initial_iterated_policy` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cdd94a-31fb-4874-8e3a-20d8bd5ad80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_max = 100;\n",
    "initial_iterated_policy = build(mysimplemodel, k_max);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1069118d-f1b7-4ee6-9962-081a262b4ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySimpleGamePolicy(Dict(5 => 0.010101010101010102, 56 => 0.010101010101010102, 35 => 0.010101010101010102, 55 => 0.010101010101010102, 60 => 0.010101010101010102, 30 => 0.010101010101010102, 32 => 0.010101010101010102, 6 => 0.010101010101010102, 67 => 0.010101010101010102, 45 => 0.010101010101010102…))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_iterated_policy.π[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844364e-0674-479c-a7f6-3afd5f1ebc74",
   "metadata": {},
   "source": [
    "### Solve\n",
    "Now that we have a `initial_iterated_policy`, and the `mysimplemodel`, we can solve the problem using the `solve(...)` method shown above. The `solve(...)` method iteratively updates the initial policy and returns the `updated_policy` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5bfc31-8325-47e0-8933-ed0ed2009218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{MySimpleGamePolicy}:\n",
       " MySimpleGamePolicy(Dict(2 => 1.0))\n",
       " MySimpleGamePolicy(Dict(2 => 1.0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_policy = VLDecisionsPackage.solve(initial_iterated_policy, mysimplemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86eb53-aed3-4543-99cf-a5494d9f44c0",
   "metadata": {},
   "source": [
    "## Hmmm. Something interesting\n",
    "The `solve(...)` returns the correct [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) value, but what happens if we use another approach, for example the `softmax_response_policy(...)` method:\n",
    "\n",
    "```julia\n",
    "function softmax_response_policy(𝒢::MySimpleGameModel, π, i, λ)\n",
    "    𝒜ᵢ = 𝒢.𝒜[i];\n",
    "    U(aᵢ) = utility(𝒢, joint(π, MySimpleGamePolicy(aᵢ), i), i);\n",
    "    return MySimpleGamePolicy(aᵢ => exp(λ*U(aᵢ)) for aᵢ in 𝒜ᵢ)\n",
    "end\n",
    "```\n",
    "\n",
    "Does this return the correct value? To explore this question, let's initialize a uniform joint policy $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b8d7c9-f48b-456d-875b-3474abe3ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "π = [MySimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in mysimplemodel.𝒜];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8262e31e-9397-4bcf-bece-c32080aa1608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{MySimpleGamePolicy}:\n",
       " MySimpleGamePolicy(Dict(5 => 0.010101010101010102, 56 => 0.010101010101010102, 35 => 0.010101010101010102, 55 => 0.010101010101010102, 60 => 0.010101010101010102, 30 => 0.010101010101010102, 32 => 0.010101010101010102, 6 => 0.010101010101010102, 67 => 0.010101010101010102, 45 => 0.010101010101010102…))\n",
       " MySimpleGamePolicy(Dict(5 => 0.010101010101010102, 56 => 0.010101010101010102, 35 => 0.010101010101010102, 55 => 0.010101010101010102, 60 => 0.010101010101010102, 30 => 0.010101010101010102, 32 => 0.010101010101010102, 6 => 0.010101010101010102, 67 => 0.010101010101010102, 45 => 0.010101010101010102…))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e6bb-aaaf-4c7d-971d-218367b791e2",
   "metadata": {},
   "source": [
    "and then call the `softmax_response_policy(...)` method with the `mysimplemodel` instance, the joint policy $\\pi$ and the index of the player $i$. Save the result in the `best` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f261fd8-f5ba-449f-acef-da9f95d5ced2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySimpleGamePolicy(Dict(5 => 1.1076263008025374e-20, 56 => 1.0817654452389634e-5, 35 => 1.7528040374050736e-10, 55 => 7.0776154263326735e-6, 60 => 5.336414438554658e-5, 30 => 6.5768684714450176e-12, 32 => 2.5203756712355084e-11, 6 => 2.8053047745044555e-20, 67 => 0.0005906239207982464, 45 => 5.8364774284405845e-8…))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1;\n",
    "λ = 1.0\n",
    "best = softmax_response_policy(mysimplemodel, π, i, λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a68658b5-66f1-4fa1-be2a-ef607e414e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.416704032495909e-22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.p[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "597f18ce-aa92-49bf-bec2-c629c0b48d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0647409478339408"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.p[97]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf7925-7393-4ed0-b182-483cb303011f",
   "metadata": {},
   "source": [
    "### Why are we not getting the Nash equilibrium value?\n",
    "Fill me in here (and below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d45e3016-1279-4f0e-9c0a-c38feaf62b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_iterations = 105\n",
    "# πᵢ = [MySimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in mysimplemodel.𝒜];\n",
    "# for _ ∈ 1:number_of_iterations\n",
    "#     πᵢ = [best_response_policy(mysimplemodel, πᵢ, i) for i in mysimplemodel.ℐ]\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a39d3b1-9d53-4e92-8897-6af02410c966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value = 2.0, with p = 1.0\n"
     ]
    }
   ],
   "source": [
    "# i = 1\n",
    "# number_of_values = length(πᵢ[i].p);\n",
    "# tmp_array = Array{Float64,2}(undef, number_of_values+1,2)\n",
    "# fill!(tmp_array,0.0);\n",
    "# for (key, value) ∈ πᵢ[i].p\n",
    "#     tmp_array[key,1] = key\n",
    "#     tmp_array[key,2] = value\n",
    "# end\n",
    "# best_index = argmax(tmp_array[:,2])\n",
    "# best_prob = tmp_array[best_index,2]\n",
    "# best_value = tmp_array[best_index,1]\n",
    "# println(\"The best value = $(best_value), with p = $(best_prob)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f25aa4ef-2cb7-4e19-916b-1f33d500f663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_iterations = 198\n",
    "πᵢ = [MySimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in mysimplemodel.𝒜];\n",
    "for _ ∈ 1:number_of_iterations\n",
    "    πᵢ = [softmax_response_policy(mysimplemodel, πᵢ, i, 7.0) for i in mysimplemodel.ℐ]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9af781b6-853a-4c5a-b189-78f810c9a35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{MySimpleGamePolicy}:\n",
       " MySimpleGamePolicy(Dict(5 => 9.79923057043117e-260, 56 => 1.7751042903243773e-102, 35 => 3.1326842375742524e-167, 55 => 1.4646472892568732e-105, 60 => 3.829897576417077e-90, 30 => 1.1980157132436041e-182, 32 => 1.759722379826469e-176, 6 => 1.1876344806723623e-256, 67 => 1.4710346761399037e-68, 45 => 2.142026488319392e-136…))\n",
       " MySimpleGamePolicy(Dict(5 => 9.79923057043117e-260, 56 => 1.7751042903243773e-102, 35 => 3.1326842375742524e-167, 55 => 1.4646472892568732e-105, 60 => 3.829897576417077e-90, 30 => 1.1980157132436041e-182, 32 => 1.759722379826469e-176, 6 => 1.1876344806723623e-256, 67 => 1.4710346761399037e-68, 45 => 2.142026488319392e-136…))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πᵢ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9feb2e59-af5e-411a-b273-991ec9f27375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value = 89.0, with p = 0.9983252803168193\n"
     ]
    }
   ],
   "source": [
    "i = 1;\n",
    "number_of_values = length(πᵢ[i].p);\n",
    "tmp_array = Array{Float64,2}(undef, number_of_values+1,2)\n",
    "fill!(tmp_array,0.0);\n",
    "for (key, value) ∈ πᵢ[i].p\n",
    "    tmp_array[key,1] = key\n",
    "    tmp_array[key,2] = value\n",
    "end\n",
    "best_index = argmax(tmp_array[:,2])\n",
    "best_prob = tmp_array[best_index,2]\n",
    "best_value = tmp_array[best_index,1]\n",
    "println(\"The best value = $(best_value), with p = $(best_prob)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e70de0e-2c37-4b28-8e7f-cb64d702e82e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×2 Matrix{Float64}:\n",
       "   0.0  0.0\n",
       "   2.0  3.30728e-265\n",
       "   3.0  3.62687e-262\n",
       "   4.0  3.97735e-259\n",
       "   5.0  4.36169e-256\n",
       "   6.0  4.78318e-253\n",
       "   7.0  5.24539e-250\n",
       "   8.0  5.75227e-247\n",
       "   9.0  6.30813e-244\n",
       "  10.0  6.91771e-241\n",
       "  11.0  7.58619e-238\n",
       "  12.0  8.31926e-235\n",
       "  13.0  9.12318e-232\n",
       "   ⋮    \n",
       "  89.0  0.998147\n",
       "  90.0  0.000922121\n",
       "  91.0  8.46405e-7\n",
       "  92.0  8.40994e-7\n",
       "  93.0  8.41019e-7\n",
       "  94.0  8.41039e-7\n",
       "  95.0  8.41054e-7\n",
       "  96.0  8.41063e-7\n",
       "  97.0  8.41068e-7\n",
       "  98.0  8.41068e-7\n",
       "  99.0  8.41063e-7\n",
       " 100.0  8.41054e-7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eceb17-5f84-46fc-93a3-bc5e0d536fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
